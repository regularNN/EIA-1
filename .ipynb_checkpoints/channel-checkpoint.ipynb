{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7083020b",
   "metadata": {},
   "source": [
    "# This cell has two classes:\n",
    "1. `EnvChannel`: Defines the environment\n",
    "2. `ControlAgent`: Defines the control agent\n",
    "\n",
    "# Parameters:\n",
    "1. ```alpha```: Learning rate of agent\n",
    "2. `gamma`: Discount factor\n",
    "3. `epsilon`: for epsilon-greedy policy\n",
    "4. `num_states`: Number of states in system, currently three(Good, Medium, Bad)\n",
    "5. `d1`: Delay threshold for Good state and medium state\n",
    "6. `d2`: Delay threshold for Medium and bad state\n",
    "\n",
    "# Return:\n",
    "1. ```action (int)```: -1: Decrease Resolution; 0: No change; 1: Increase Resolution\n",
    "2. `state (int)`: 0: Bad; 1: Medium; 2: Good\n",
    "\n",
    "\n",
    "# Arguments:\n",
    "1. `state_list`: List of integer states. \n",
    "2. `avg_confidence`: (Float) Average confidence \n",
    "    \n",
    "# Sample use:\n",
    "\n",
    "Initialize ControlAgent class:\n",
    "```\n",
    "agent = ControlAgent(resolution_list=[100,200,300], d1=0.00103,d2=0.00161)\n",
    "```\n",
    "To train and get optimal actions\n",
    "```\n",
    "action = agent.get_signal(delay_list = [.5,.6], curr_resolution=200, error_score=.5)\n",
    "```\n",
    "\n",
    "To get random actions:\n",
    "```\n",
    "action = agent.get_signal(delay_list, avg_confidence, random_actions=True)\n",
    "```\n",
    "\n",
    "# To Do:\n",
    "1. ~~Instead of taking state_list directly, estimate the states indirectly from packet delay.~~ **Done**\n",
    "2. ~~Update the function get_delay_factor.~~ **Done** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d24cbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "class EnvChannel:\n",
    "    def __init__(self, resolution_list, d1=.01, d2=.02):\n",
    "\n",
    "        self.num_delay_bins = 3\n",
    "        self.num_resolutions = len(resolution_list)\n",
    "\n",
    "        self.resolution_dict = {resolution: i for i, resolution in enumerate(resolution_list)}\n",
    "        \n",
    "\n",
    "        self.num_states = self.num_resolutions*self.num_delay_bins\n",
    "        self.curr_state = (0,0)\n",
    "        self.prev_state = (0,0)\n",
    "        self.reward = 0\n",
    "        self.action = 0\n",
    "        self.num_actions = 3\n",
    "        self.valid_actions = [0, 1, 2]  # Reduce, No change, Increase\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        self.avg_delay = 0\n",
    "        self.curr_resolution = 0\n",
    "        self.error_score=  0\n",
    "\n",
    "    def sample_action(self):\n",
    "        return np.random.choice(self.valid_actions)\n",
    "    \n",
    "\n",
    "    def get_reward(self):\n",
    "        return 0.5*self.resolution_state/(self.delay_state+1) +0.5*self.error_score\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        self.reward = self.get_reward()\n",
    "        return self.reward, self.estimate_state()\n",
    "\n",
    "    \n",
    "    def estimate_state(self):\n",
    "        self.resolution_state = self.resolution_dict[self.curr_resolution]\n",
    "        if self.avg_delay <= self.d1:\n",
    "            self.delay_state = 0  # Good state\n",
    "        elif self.avg_delay <= self.d2:\n",
    "            self.delay_state = 1  # Medium state\n",
    "        else:\n",
    "            self.delay_state = 2  # Bad state\n",
    "        self.curr_state = (self.delay_state, self.resolution_state)\n",
    "        return self.curr_state\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(num_states=self.num_states)\n",
    "        return self.curr_state\n",
    "\n",
    "\n",
    "class ControlAgent:\n",
    "    def __init__(self,\n",
    "                 resolution_list,\n",
    "                 d1,\n",
    "                 d2,\n",
    "                 alpha=0.1,\n",
    "                 gamma=.1,\n",
    "                 epsilon=.3,\n",
    "                 random_actions=False):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.env = EnvChannel(resolution_list, d1, d2)\n",
    "        self.q_table = np.zeros([self.env.num_delay_bins, self.env.num_resolutions, self.env.num_actions])\n",
    "        self.all_epochs = []\n",
    "        self.action_record = []\n",
    "        self.state_record = []\n",
    "        self.penalties = []\n",
    "        self.iteration_i = 0\n",
    "        self.prev_state = None\n",
    "        self.prev_avg_delay = 0\n",
    "        self.random_actions = random_actions\n",
    "        \n",
    "  \n",
    "    def map_action(self, state):\n",
    "        if state == 0:\n",
    "            return -1\n",
    "        if state == 1:\n",
    "            return 0\n",
    "        if state == 2:\n",
    "            return 1\n",
    "        \n",
    " \n",
    "    def get_signal(self, delay_list, curr_resolution, error_score):\n",
    "        self.env.avg_delay = np.average(delay_list)\n",
    "        self.env.curr_resolution = curr_resolution\n",
    "        self.env.error_score = np.abs(error_score)/ 100.0\n",
    "        \n",
    "        self.iteration_i += 1\n",
    "        if np.mod(self.iteration_i+2, 10) == 0:\n",
    "            self.alpha = self.alpha*.95\n",
    "            self.epsilon = self.epsilon*.95\n",
    "            \n",
    "\n",
    "        if self.iteration_i == 1:\n",
    "            state = self.env.estimate_state()\n",
    "            action = self.env.sample_action()\n",
    "            reward = 0 \n",
    "\n",
    "        else:\n",
    "  \n",
    "            reward, state = self.env.step(self.prev_action)\n",
    "            self.state_record.append(self.env.curr_state)\n",
    "            \n",
    "\n",
    "            if not self.random_actions:\n",
    "                old_qvalue = self.q_table[self.prev_state[0],self.prev_state[1], \n",
    "                                          self.prev_action]\n",
    "                next_max = np.max(self.q_table[state[0],state[1], :])\n",
    "\n",
    "                new_qvalue = (1 - self.alpha) * old_qvalue + \\\n",
    "                    self.alpha * (reward + self.gamma * next_max)\n",
    "                self.q_table[self.prev_state[0],self.prev_state[1], \n",
    "                             self.prev_action] = new_qvalue\n",
    "                self.penalties.append(reward)\n",
    "                self.action_record.append(self.prev_action)\n",
    "                \n",
    "\n",
    "                if np.random.uniform(0, 1) < self.epsilon:\n",
    "                    action = self.env.sample_action()  # Explore action space\n",
    "\n",
    "                else:\n",
    "                    # Exploit learned values\n",
    "                    action = np.argmax(self.q_table[state[0],state[1], :])\n",
    "            else:\n",
    "                action = self.env.sample_action()  # Explore action space\n",
    "\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return self.map_action(action), action, state, reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "809a457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING\n",
    "import pandas as pd \n",
    "d1 = 0.00103\n",
    "d2 = 0.00161\n",
    "agent = ControlAgent(d1,d2, gamma=0.0)\n",
    "f_name = \"../datare.csv\"\n",
    "df =pd.read_csv(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "582b2664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 1 0\n",
      "0 -1 -2\n",
      "1 -1 0\n",
      "-1 -1 -2\n",
      "-1 -1 2\n",
      "-1 -1 2\n",
      "0 -1 2\n",
      "0 0 0\n",
      "-1 1 0\n",
      "1 1 -2\n",
      "1 1 2\n",
      "0 1 2\n",
      "1 1 0\n",
      "-1 -1 2\n",
      "0 -1 2\n",
      "0 -1 0\n",
      "0 -1 0\n",
      "0 -1 0\n",
      "0 -1 0\n",
      "1 1 0\n",
      "0 -1 2\n",
      "0 0 0\n",
      "1 1 0\n",
      "0 -1 2\n",
      "-1 -1 0\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "-1 -1 2\n",
      "0 -1 2\n",
      "-1 -1 0\n",
      "0 -1 2\n",
      "1 0 0\n",
      "-1 -1 0\n",
      "0 0 2\n",
      "-1 -1 0\n",
      "0 0 2\n",
      "0 -1 0\n",
      "1 1 0\n",
      "0 -1 2\n",
      "1 -1 0\n",
      "1 1 -2\n",
      "0 -1 2\n",
      "1 1 0\n",
      "1 1 2\n",
      "-1 -1 2\n",
      "1 1 2\n",
      "-1 -1 2\n",
      "-1 -1 2\n",
      "-1 -1 2\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.DataFrame(columns=['State', 'Action', 'Delay'])\n",
    "len_df = len(df)\n",
    "len_df = 50\n",
    "\n",
    "tmp_delay = []\n",
    "reward = 0\n",
    "reward_list = []\n",
    "for j in range(len_df):\n",
    "    i = np.mod(j, 99)\n",
    "#     print(i)\n",
    "    delay = df['Delay'][i]\n",
    "    conf = df['Conf_score'][i]\n",
    "    action, state, rew = agent.get_signal(delay, conf)\n",
    "    print(state, action, rew)\n",
    "#     print(agent.q_table)\n",
    "    reward += rew\n",
    "    if np.mod(j, 9)==0:\n",
    "        reward_list.append(np.average(reward))\n",
    "        reward = 0\n",
    " \n",
    "#     df_new.loc[i] = [state, action, delay]\n",
    "# df_new.to_csv(\"data_my_.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
