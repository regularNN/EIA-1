{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7083020b",
   "metadata": {},
   "source": [
    "# This cell has two classes:\n",
    "1. `EnvChannel`: Defines the environment\n",
    "2. `ControlAgent`: Defines the control agent\n",
    "\n",
    "# Parameters:\n",
    "1. ```alpha```: Learning rate of agent\n",
    "2. `gamma`: Discount factor\n",
    "3. `epsilon`: for epsilon-greedy policy\n",
    "4. `num_states`: Number of states in system, currently three(Good, Medium, Bad)\n",
    "5. `d1`: Delay threshold for Good state and medium state\n",
    "6. `d2`: Delay threshold for Medium and bad state\n",
    "\n",
    "# Return:\n",
    "1. ```action (int)```: -1: Decrease Resolution; 0: No change; 1: Increase Resolution\n",
    "2. `state (int)`: 0: Bad; 1: Medium; 2: Good\n",
    "\n",
    "\n",
    "# Arguments:\n",
    "1. `state_list`: List of integer states. \n",
    "2. `avg_confidence`: (Float) Average confidence \n",
    "    \n",
    "# Sample use:\n",
    "\n",
    "Initialize ControlAgent class:\n",
    "```\n",
    "agent = ControlAgent(d1=10,d2=20)\n",
    "```\n",
    "To train and get optimal actions\n",
    "```action = agent.get_signal(delay_list, avg_confidence)```\n",
    "\n",
    "To get random actions:\n",
    "```action = agent.get_signal(delay_list, avg_confidence, random_actions=True)```\n",
    "\n",
    "# To Do:\n",
    "1. ~~Instead of taking state_list directly, estimate the states indirectly from packet delay.~~ **Done**\n",
    "2. ~~Update the function get_delay_factor.~~ **Done** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24cbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "class EnvChannel:\n",
    "    def __init__(self, num_states=3, d1=.01, d2=.02):\n",
    "\n",
    "        self.num_states = num_states\n",
    "        self.states = np.arange(self.num_states)\n",
    "        self.curr_state = np.random.choice(self.states)\n",
    "        self.prev_state = -1\n",
    "        self.reward = 0\n",
    "        self.action = 0\n",
    "        self.num_actions = 3\n",
    "        self.valid_actions = [-1, 0, 1]  # Reduce, No change, Increase\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        self.avg_delay = 0\n",
    "\n",
    "    def sample_action(self):\n",
    "        return np.random.randint(self.num_actions)\n",
    "\n",
    "    def step(self, action, partial_reward):\n",
    "        self.action = action\n",
    "        self.reward = -10 * self.avg_delay + partial_reward\n",
    "        return self.reward\n",
    "\n",
    "    def estimate_state(self, delay_list):\n",
    "        self.avg_delay = np.sum(delay_list) / np.max(np.shape(delay_list))\n",
    "        if self.avg_delay <= self.d1:\n",
    "            self.curr_state = 2  # Good state\n",
    "        elif self.avg_delay <= self.d2:\n",
    "            self.curr_state = 1  # Medium state\n",
    "        else:\n",
    "            self.curr_state = 0  # Bad state\n",
    "        return self.curr_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__(num_states=self.num_states)\n",
    "        return self.curr_state\n",
    "\n",
    "\n",
    "class ControlAgent:\n",
    "    def __init__(self,\n",
    "                 d1,\n",
    "                 d2,\n",
    "                 alpha=0.1,\n",
    "                 gamma=.99,\n",
    "                 epsilon=.95,\n",
    "                 num_states=3,\n",
    "                 random_actions=False):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.env = EnvChannel(num_states, d1, d2)\n",
    "        self.q_table = np.zeros([self.env.num_states, self.env.num_actions])\n",
    "        self.all_epochs = []\n",
    "        self.penalties = 0\n",
    "        self.iteration_i = 0\n",
    "        self.prev_state = None\n",
    "        self.prev_avg_delay = 0\n",
    "        self.random_actions = random_actions\n",
    "\n",
    "    def get_delay_factor(self):\n",
    "        # This should be function of delay\n",
    "        #         return np.average(delays)\n",
    "        return self.avg_delay\n",
    "\n",
    "    def get_signal(self, delay_list, avg_confidence):\n",
    "\n",
    "        state = self.env.estimate_state(delay_list)\n",
    "        self.iteration_i += 1\n",
    "\n",
    "        if self.iteration_i == 1:\n",
    "            self.prev_state = state\n",
    "            action = self.env.sample_action()\n",
    "\n",
    "        else:\n",
    "            #delay_factor = self.get_delay_factor()\n",
    "            reward = self.env.step(self.prev_action, avg_confidence)\n",
    "\n",
    "            if not self.random_actions:\n",
    "                old_qvalue = self.q_table[self.prev_state, self.prev_action]\n",
    "                next_max = np.argmax(self.q_table[state, :])\n",
    "\n",
    "                new_qvalue = (1 - self.alpha) * old_qvalue + \\\n",
    "                    self.alpha * (reward + self.gamma * next_max)\n",
    "                self.q_table[self.prev_state, self.prev_action] = new_qvalue\n",
    "\n",
    "                self.penalties += reward\n",
    "\n",
    "                if np.random.uniform(0, 1) < self.epsilon:\n",
    "                    action = self.env.sample_action()  # Explore action space\n",
    "\n",
    "                else:\n",
    "                    # Exploit learned values\n",
    "                    action = np.argmax(self.q_table[state, :])\n",
    "            else:\n",
    "                action = self.env.sample_action()  # Explore action space\n",
    "\n",
    "            if self.iteration_i % 100 == 0:\n",
    "                with open(f\"data/iteration_{self.iteration_i}\", \"wb\") as fp:\n",
    "                    pickle.dump([self.q_table, self.penalties], fp)\n",
    "\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        #         self.prev_avg_delay = self.avg_delay\n",
    "        action = self.env.valid_actions[action]\n",
    "        #         print(f\"Action is: {action}\")\n",
    "        return action, state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
